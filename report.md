**CMSI 370** Interaction Design, Fall 2017
# Usability Study of Puzzle Games

- Simon Wroblewski
- Emily Shoji
- Joey Martinez
- Jordan Sanders

## Study Description

- We will focus on puzzle games accessible on handheld devices, and test errors, learnability, and satisfaction. We are going to use 2048, flow free & 1010!
- State the tasks that were chosen:

### Task 1: Play the game (to test learnability)
After a brief description of how the game works, we will have the test subject play a few rounds (in Flow Free), or play continuously for a few minutes (for 2048 and 1010!) and time how long it takes until they feel confident in their ability to play the game.

### Task 2: Testing Errors
Have the test subject play for about 10 minutes and count the number of errors made that deter from the goal of the game. We will also record the error made to see if there is a common type of error made while playing the game.

### Task 3: Ask users to rate out of 1-5 how much they enjoyed the game
Record any specific aspects of the game the test subject did or did not like. (ex. too many ads, too repetitive)

## Study Results
For the project, we used a [Google Form](https://goo.gl/forms/sA1XMlcLUP8G2iYl2) for data collection.

### Flow Free Results 
![](/screenshots/FlowFreeEnjoyment.png)

- The average time it took for our players to learn Flow Free was 50 seconds
- The average number of errors made while playing was ~1.778

Some comments made about the game were:
* Switching between levels and continuing to the next level was easy
* Ads every few levels were annoying
* Almost too easy of a concept

### 2048 Results
![](/screenshots/2048Enjoyment.png)

- The average time it took for our players to feel confident playing 2048 was 1 minute and 8 seconds
- The average number of errors made while playing was ~1.222

Some comments made about the game were:
* Ads had hard to find exits
* The color scheme was not inviting or exciting, which detracted from the game
* As the numbers on the tiles got bigger, it felt like the player was being rewarded

### 1010! Results
![](/screenshots/1010Enjoyment.png)

- The average time it took for our players to feel confident playing 1010! was 1 minute and 9.44 seconds
- The average number of errors made while playing was ~2.11

Some comments made about the game were:

* Compared to Tetris, not as fun
* There was no tutorial but the game was easy to pick up
* Stars felt like a good reward
* Watching ads gave more stars which was better than just being stuck on an ad


## Heuristic Evaluation

!["1010!"](/screenshots/playing1010.PNG) ![2048](/screenshots/2048.PNG) ![Flow Free](/screenshots/flow1.PNG)

The three games we decided to use for our experiment for this exercise were 1010!, 2048, and Flow Free. These games were chosen specifically because they are all similar in the way that they play, but all take a different approach to this style of playing. The three all used the direct manipulation and menu selection interaction styles which allows us to justifiably compare their good and bad qualities with respect to user interaction. The three metrics we decided to choose for these three games were learnability, errors, and satisfaction. Learnability was chosen because we were curious as to how each game differed and whether or not one of them was faster to learn than the others, and why this might be the case. Learnability is especially important within games because the faster a game can be learned, the more likely a person will be to continue playing it. Errors was chosen because it seemed that each game had its own possibility for errors, and we were curious to see which errors occurred more than others. Minimizing errors is important for a game, as it will allow the player to continue making more favorable outcomes, which might make their experience more enjoyable. For satisfaction, we wanted to see which aspects of the game were most satisfying, as satisfaction in a game is one of the most important parts when a consumer is deciding whether or not they want to continue playing.

After testing numerous people, we found that Flow Free performed the best out of the three games in two of the metrics. In terms of minimal errors, we found that Flow Free’s design allowed for a vast reduction in errors. This design works because while playing the game, all of the buttons that the player is expected to press are big enough and close enough that it makes hitting them consistently quite easy. This works well with Fitt’s law, as it follows its understood notion that the bigger and closer a button is, the easier it is to hit. In this case, since players are missing less often, they are making far fewer errors, than what was seen in the other games. Likewise, there was constant feedback whenever a player performed an action. Since all of the menus were clearly labeled, it was hard for a player to make the error of moving to the wrong place in the game. However, even in the event when they did, they were able to move back to where they really wanted to go with ease, as the place they ended up in still had clear menus or clearly marked exits that could move the user to where they wanted to go. This demonstrates how Flow Free has good mapping within its design (Nielsen, Jakob (1957). *Usability Engineering* pg 126-129). 

Similarly, these principles also greatly helped contribute to Flow Free’s fast learnability. With menus that are clear, direct, and hierarchical, Flow Free is not only less error prone, but it is also quite easy to navigate. Shortcuts also exist in many places as well, making the player feel comfortable with moving through the game, without having to constantly move back to a main menu to find where they want to go. Flow Free's menus are a great example of the guideline "Hierarchical Navigation" in the *iOS Human Interface Guidelines*. Flow Free follows the theory of Consistency through grammar as well, as it uses the same words for every place that can be navigated through within the app, making it easy to navigate the menu, no matter where you are. Surprisingly, however, despite lacking a tutorial for the game itself within the app, players were able to understand how to play with the least amount of time between all of our games. This is because the other games had far more moves that could be considered errors, whereas Flow Free has far fewer moves to make that would be considered errors. This simple design followed Jakob Nielsen's principle of “less is more” and allowed players to feel comfortable with their knowledge of the game only after a short time (Nielsen, Jakob (1957). *Usability Engineering* pg 120-123). Whenever a player made a move, there was instant feedback that would allow the player to get a sense of whether this was a good or a bad move. This was not present in the other games, as those games forced the player to make a judgement as to whether their move was favorable or not, without any obvious feedback. Feedback was not only present while playing the game, but also when finishing a level. Once all of the correct moves were made, the players were greeted with a screen that told them of their success, whereas the other games only greeted the players with a screen when they had made enough errors to get a game over. The players in the other two games were only aware of the immediate moves that got them to a game over screen, but not necessarily what would have been favorable moves that could have avoided failure, or even better, moved toward success. This is an example of the feedback guideline "Don’t rely on a single mode of communication" found in the *iOS Human Interface Guidelines*.
 
However, in terms of satisfaction, we found that 1010! ranked the highest out of all of our games. We believe the reason for this was because it also had a simple, “less is more” design similar to Flow Free. Different from Flow Free however, is that it took less is more to a slightly higher degree. There are less menus to navigate through as compared to Flow Free, and while Flow Free had pop-up menus, 1010! has menus that are part of the game screen, so that you almost never leave the game space where the game will be played, unless the player is specifically moving back to the main menu. This can be somewhat confusing at first, but once the players understood this, it became obvious that they could get back to the game within the app, without having to worry about moving through menus. This was satisfying as it allowed the players to just quickly get back to the main feature of the app, which is the game itself. It also had a quite modern look, with a color scheme that used a lot of dulled primary colors, and rounded game tiles which gave it a simple and elegant look that was satisfying to look at. There was also a large satisfaction drawn from planning out ahead of time, and then successfully playing a favorable move, that allowed the player to get a sense that they were accomplishing what they thought to be a difficult task. In this same sense, they also got a sense of satisfaction from organizing the tiles that were out of order, or in what felt to be less than favorable places. Some people even likened it to the same feeling as the feeling one gets after cleaning their room. Interestingly, we also found that because many people had domain knowledge of a similar game, Tetris, many people got satisfaction from a new take on an idea that they already had in their mind. Even though this prior knowledge forced the player to start making incorrect assumptions on how the game was played, and this added to their errors and time it took to play the game. The players often mentioned that the game was satisfying because it had a new spin on a game that they already knew. Once they learned how to play, it was clear that players tended to have a quite strong conceptual model of the game, despite having taken a longer time to learn the game.

The final app we used seemed to underperform in the three metrics that we chose. In terms of learnability, 2048 seemed to have the weakest conceptual model. Because the game was direct manipulation, many people believed that they could simply move the tiles in the game to any place on screen. Instead, after trying to move a particular tile, many players were greeted with a gulf of execution. Many of our players believed that they were going to move a single tile one space, but instead, the all the tiles moved across the screen. Since the mental model was not strong within the user, it took longer for the users to learn how to play the game accurately. According to Apple’s iOS guidelines, an app should work as a metaphor if possible. By metaphor, Apple implies that when performing actions, the event should replicate familiar experiences. In this case, the metaphor of swiping to move tiles to a particular location was present in most users, but since this metaphor is not accurate to the application, it makes learning the app take much longer. The players we tested also noted that the game did not have a very pleasing color scheme. This resulted in the game scoring lower in satisfaction. The results showed that most users were content with the game, but did not enjoy it as much as the other games with more colors.

Furthermore, while all of the metrics we decided to focus on for this assignment are important, we found that in the particular field of video game apps, some metrics are slightly more important than others. We found that errors were the least important metric in this particular field, because users are generally more willing to accept the errors they made because a phone is a somewhat small space to work in, and so some of the finer details when playing the games were lost. These errors can be seen as somewhat of an overarching issue with playing games on a phone in general, so users come in with an understanding that errors are more forgivable.

The second most important metric was learnability. Because games on a phone are supposed to be something that can be played on the go, and played anywhere at anytime, they should be easy to pick up. Games that have a high learning curve are usually seen as too difficult for a mobile game, and this discourages people from playing them. However, a game that is simple and easy to pick up will be more likely to gather a larger group of people who will be willing to continue playing it. 

However, the most important metric for mobile games was definitely satisfaction. We found that despite how long a player took to learn a game, or how many errors they made, if the game simply was not satisfying, then players would be the most likely to not return to the game, or continue playing it for extended periods of time. An app developer should try to minimize the amount of errors a player makes, and make it a quick game to learn, but if both of these things are done well, but the game is not satisfying, then the developer has not prioritized what they should be focusing on correctly. The color scheme of the game also seemed to impact the level of satisfaction our users experienced while playing the three puzzle games. The Apple developer website gives an example color scheme, shown below, which is clearly followed by 1010!.
 
![Example Color Scheme](/screenshots/exampleColors.png)
 
Apple also encourages users to stay within standard RGB values and only venture outside of those values to increase depth or experience on wide displays.
 
![Standard RGB](/screenshots/sRGB.png)
 
 Since 1010! followed these guidelines, we believe that is the reason why it ranked so much higher in satisfaction compared to 2048.
 
Despite Flow Free not ranking the highest in satisfaction, we believe that Flow Free performed the best overall with respect to usability. Its menu style design lent itself to being highly learnable in a short period of time, as it allowed for clearly marked exits, feedback, consistency, and shortcust to stand out, which are four of Jakob Nielsen's *Ten Usability Heuristics*. The players consistently knew where they were and where they were going whenever they were using the app at any given point and time. This ease of access reduced errors, and time it took to learn how to navigate the interface. Within the game itself, using Fitt’s law, it can be understood that the buttons are more easy to press and move in Flow Free than in the other games, as Flow Free had buttons that were spread across the upper middle of the screen, and sized in a way that made them harder to miss. Flow Free also had clear consistency throughout its entire interface. No matter what actions were occurring, the user had consistent feedback to indicate that the player had done something, whether it was making a move in the game, or navigating a menu. It was quite rare to see a player confused as to what occurred after they performed an action, making Flow Free the best performer within the 7 stages of action. We experienced few players that had either a gulf of evaluation or execution, as both were handled well within the feedback and the menu system. In short, a user could almost always form an action they wanted to perform, perform this action, and their perceived result of this action was much in line with what the interface was trying to portray. Because this model is so strong within the user, learnability was the fastest, and errors were the most reduced, we felt that Flow Free was the most easily usable out of the three apps that we tested.

## Statement of Work
- Joey: Wrote bulk of the heuristic evalutation, help with citations, carried out tests
- Emily: Worked on taking screenshots of apps and data, compiling and summarizing results, carried out tests
- Simon: Helped write the heurisitic evaluation with Joey, provided citations for important ideas, carried out tests
- Jordan: Helped find sources and write the Usability Metrics evaluation, reviewed work, carried out tests
